{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usvenkat/AI-Agent_Developer-2/blob/main/Web_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVkPbdh79FD6",
        "outputId": "22b96ae2-85e4-44bd-b46e-7150d3c02af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "pip install googlesearch-python requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from googlesearch import search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import webbrowser\n",
        "import time\n",
        "from IPython.display import display, HTML\n"
      ],
      "metadata": {
        "id": "QOhq_trZ950a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_topic_info(topic):\n",
        "\n",
        "    important_points = []\n",
        "    time.sleep(2)\n",
        "    print(f\"Searching the web for: {topic}...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        for url in search(topic, num_results=5, lang='en'):\n",
        "            print(f\"Checking general info URL: {url}\")\n",
        "            try:\n",
        "                response = requests.get(url, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                paragraphs = soup.find_all('p')\n",
        "                for p in paragraphs[:3]:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 50:\n",
        "                        important_points.append(text)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Could not access {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "\n",
        "            if len(important_points) > 10:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during web search for general info: {e}\")\n",
        "\n",
        "    return important_points\n",
        "\n"
      ],
      "metadata": {
        "id": "0Y99ZvP_Sif-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_display_image(topic):\n",
        "\n",
        "    print(f\"\\nSearching Google Images for: {topic}...\")\n",
        "    image_search_query = f\"{topic} image\"\n",
        "    direct_image_url = None\n",
        "\n",
        "    try:\n",
        "        for url in search(image_search_query, num_results=5, lang='en', safe='on'):\n",
        "            if \"images.google.com\" in url or \"google.com/images\" in url:\n",
        "                print(f\"Checking image search result URL: {url}\")\n",
        "                try:\n",
        "                    response = requests.get(url, timeout=5)\n",
        "                    response.raise_for_status()\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "                    for meta in soup.find_all('meta', property=\"og:image\"):\n",
        "                        direct_image_url = meta.get('content')\n",
        "                        if direct_image_url and (direct_image_url.startswith('http') or direct_image_url.startswith('https')):\n",
        "                            print(f\"Found potential direct image URL from og:image: {direct_image_url}\")\n",
        "                            break\n",
        "\n",
        "                    if not direct_image_url:\n",
        "\n",
        "                        for img_tag in soup.find_all('img', src=True):\n",
        "                            src = img_tag.get('src')\n",
        "                            if src and (src.startswith('http') or src.startswith('https')) and ('q=tbn' not in src): # Filter out thumbnails\n",
        "                                if any(ext in src for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
        "                                    direct_image_url = src\n",
        "                                    print(f\"Found potential direct image URL from img src: {direct_image_url}\")\n",
        "                                    break\n",
        "                    if direct_image_url:\n",
        "                        break\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Could not access {url} for image: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing {url} for image: {e}\")\n",
        "            if direct_image_url:\n",
        "                break\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Google Images search: {e}\")\n",
        "\n",
        "    if direct_image_url:\n",
        "        print(f\"\\nOpening image in your default web browser: {direct_image_url}\")\n",
        "        try:\n",
        "            webbrowser.open_new_tab(direct_image_url)\n",
        "            return direct_image_url\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open browser: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"\\nCould not find a direct image URL to display.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "GSyEE7DaSpZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_info_file(topic, points, image_url=None):\n",
        "    \"\"\"\n",
        "    Creates a text file with the extracted important points and an image URL (if found).\n",
        "    \"\"\"\n",
        "    filename = f\"{topic.replace(' ', '_').lower()}_info.txt\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"--- Information about: {topic.upper()} ---\\n\\n\")\n",
        "            f.write(\"Important Points:\\n\")\n",
        "            if points:\n",
        "                for i, point in enumerate(points):\n",
        "                    f.write(f\"{i+1}. {point}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"No significant points found.\\n\\n\")\n",
        "\n",
        "            if image_url:\n",
        "                f.write(\"\\n--- Image Displayed (Opened in Browser) ---\\n\")\n",
        "                f.write(f\"The image was opened in your default web browser. You can also view it at:\\n{image_url}\\n\")\n",
        "            else:\n",
        "                f.write(\"No direct image was found or opened in browser.\\n\")\n",
        "\n",
        "        print(f\"\\nInformation saved to: {filename}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file {filename}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qOUzXvxlSpcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_info_from_web(query, num_results=3):\n",
        "\n",
        "    print(f\"Searching the web for: {query}\")\n",
        "    extracted_info = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        search_results = search(query, num_results=num_results, lang='en')\n",
        "\n",
        "        for url in search_results:\n",
        "            print(f\"Processing URL: {url}\")\n",
        "            try:\n",
        "\n",
        "                response = requests.get(url, timeout=5)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "                paragraphs = soup.find_all('p')\n",
        "                page_text = \"\"\n",
        "                for p in paragraphs[:5]:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if len(text) > 100:\n",
        "                        page_text += text + \"\\n\\n\"\n",
        "\n",
        "                if page_text:\n",
        "                    extracted_info.append({\"url\": url, \"content\": page_text})\n",
        "                else:\n",
        "                    extracted_info.append({\"url\": url, \"content\": \"No significant text found on this page.\"})\n",
        "\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Could not access {url}: {e}\")\n",
        "                extracted_info.append({\"url\": url, \"content\": f\"Could not access page: {e}\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {url}: {e}\")\n",
        "                extracted_info.append({\"url\": url, \"content\": f\"Error parsing page: {e}\"})\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during web search: {e}\")\n",
        "\n",
        "    return extracted_info\n"
      ],
      "metadata": {
        "id": "ng0ZxURSX5nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def display_google_cse(cx_code=\"23780fbb727f84965\"):\n",
        "\n",
        "  google_cse_code = f\"\"\"\n",
        "  <script async src=\"https://cse.google.com/cse.js?cx={cx_code}\">\n",
        "  </script>\n",
        "  <div class=\"gcse-search\"></div>\n",
        "  \"\"\"\n",
        "\n",
        "  display(HTML(google_cse_code))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic_input = input(\"Enter the topic you want to search for: \")\n",
        "\n",
        "\n",
        "    points = get_topic_info(topic_input)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Please review the generated text file for information.\")\n",
        "    if found_image_url:\n",
        "        time.sleep(2)\n",
        "        print(\"An image related to your topic should have opened in your browser.\")\n",
        "        print()\n",
        "    information = get_info_from_web(topic_input)\n",
        "\n",
        "    print(\"\\n--- Extracted Information ---\")\n",
        "    if information:\n",
        "        for item in information:\n",
        "            print(f\"From URL: {item['url']}\")\n",
        "            print(\"Content:\")\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print(item['content'])\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "            print(\"-\" * 30)\n",
        "    else:\n",
        "        print(\"No information could be extracted.\")\n",
        "\n",
        "    print(\"Saving file .....\")\n",
        "    found_image_url = find_and_display_image(topic_input)\n",
        "    create_info_file(topic_input, points, found_image_url)\n",
        "\n",
        "    print(\"If u wanna search manually \")\n",
        "    time.sleep(2)\n",
        "    print(\"rendering the search block ........\")\n",
        "    time.sleep(2)\n",
        "    print()\n",
        "    print(\"--------------GOOGLE SEARCH----------------\")\n",
        "    print()\n",
        "    display_google_cse()\n",
        "    print()\n",
        "    print(\"-------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZwpYcQ-aSiid",
        "outputId": "85a61d21-351f-4ef4-ce13-3d1791ff47ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic you want to search for: nvidia rtx 5090\n",
            "Searching the web for: nvidia rtx 5090...\n",
            "Checking general info URL: https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/\n",
            "Checking general info URL: https://marketplace.nvidia.com/en-us/consumer/graphics-cards/gigabyte-nvidia-geforce-rtx-5090-windforce-overclocked-triple-fan/\n",
            "Could not access https://marketplace.nvidia.com/en-us/consumer/graphics-cards/gigabyte-nvidia-geforce-rtx-5090-windforce-overclocked-triple-fan/: HTTPSConnectionPool(host='marketplace.nvidia.com', port=443): Read timed out. (read timeout=5)\n",
            "Checking general info URL: https://www.amazon.com/Nvidia-GeForce-RTX-5090-Founders/dp/B0DYDY8KSC\n",
            "Checking general info URL: https://www.polygon.com/gaming/545482/rtx-5090-review-nvidia-graphics-card-gpu\n",
            "Checking general info URL: https://www.google.com/search?num=7\n",
            "\n",
            "--- Process Complete ---\n",
            "\n",
            "\n",
            "Please review the generated text file for information.\n",
            "Searching the web for: nvidia rtx 5090\n",
            "Processing URL: https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/\n",
            "Processing URL: https://marketplace.nvidia.com/en-us/consumer/graphics-cards/gigabyte-nvidia-geforce-rtx-5090-windforce-overclocked-triple-fan/\n",
            "Could not access https://marketplace.nvidia.com/en-us/consumer/graphics-cards/gigabyte-nvidia-geforce-rtx-5090-windforce-overclocked-triple-fan/: HTTPSConnectionPool(host='marketplace.nvidia.com', port=443): Read timed out. (read timeout=5)\n",
            "Processing URL: https://www.amazon.com/Nvidia-GeForce-RTX-5090-Founders/dp/B0DYDY8KSC\n",
            "\n",
            "--- Extracted Information ---\n",
            "From URL: https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Visit your regional NVIDIA website for local content, pricing, and where to buy partners specific to your country.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "From URL: https://marketplace.nvidia.com/en-us/consumer/graphics-cards/gigabyte-nvidia-geforce-rtx-5090-windforce-overclocked-triple-fan/\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Could not access page: HTTPSConnectionPool(host='marketplace.nvidia.com', port=443): Read timed out. (read timeout=5)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "From URL: https://www.amazon.com/Nvidia-GeForce-RTX-5090-Founders/dp/B0DYDY8KSC\n",
            "Content:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Found a lower price? Let us know. Although we can't match every price reported, we'll use your feedback to ensure that our prices remain competitive.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "Saving file .....\n",
            "\n",
            "Searching Google Images for: nvidia rtx 5090...\n",
            "\n",
            "Could not find a direct image URL to display.\n",
            "\n",
            "Information saved to: nvidia_rtx_5090_info.txt\n",
            "If u wanna search manually \n",
            "rendering the search block ........\n",
            "\n",
            "--------------GOOGLE SEARCH----------------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <script async src=\"https://cse.google.com/cse.js?cx=23780fbb727f84965\">\n",
              "  </script>\n",
              "  <div class=\"gcse-search\"></div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBcxS3zSZ6O0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}